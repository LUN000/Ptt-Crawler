{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>PTT標題</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = input(\"輸入英文版名:\")\n",
    "pages = input(\"查詢頁數\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\"\"\"\n",
    "無推數版本\n",
    "\"\"\"\n",
    "def get_agree(pages,r,soup):\n",
    "    button = soup.select('input')\n",
    "    agree = button[0][\"value\"]\n",
    "    agree_url = 'https://www.ptt.cc' + agree\n",
    "    url = agree_url\n",
    "    print(url)\n",
    "\n",
    "\n",
    "    \n",
    "pages = int(pages)\n",
    "url = f\"https://www.ptt.cc/bbs/{board}/index.html\"\n",
    "requests.get(url)\n",
    "\n",
    "with open(f\"ptt{board}.csv\",'w',encoding='utf-8-sig') as file: \n",
    "    for page in range(pages):\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text,\"html.parser\")\n",
    "        div_t = soup.find_all('div',class_=\"title\")\n",
    "        for t in div_t:   \n",
    "            file.write(t.text)\n",
    "        dictionary = dict(zip(tl, nl))\n",
    "        file.write(str(dictionary))\n",
    "        btn = soup.select('div.btn-group > a')\n",
    "        up_page_href = btn[3]['href']\n",
    "        next_page_url = 'https://www.ptt.cc' + up_page_href\n",
    "        url = next_page_url\n",
    "        print(f'搜尋第{page+1}頁',end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>整理</h1>\n",
    "輸出至 ptt{board}.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(f\"ptt{board}.csv\",header=None,delimiter='\\\\t',encoding='utf-8-sig')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.unicode.ambiguous_as_wide', True)\n",
    "pd.set_option('display.unicode.east_asian_width', True)\n",
    "df.to_csv(f\"ptt{board}.csv\", encoding=\"utf_8_sig\",header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\"\"\"\n",
    "含推數 \n",
    "\"\"\"\n",
    "def get_agree(pages,r,soup):\n",
    "    button = soup.select('input')\n",
    "    agree = button[0][\"value\"]\n",
    "    agree_url = 'https://www.ptt.cc' + agree\n",
    "    url = agree_url\n",
    "    print(url)\n",
    "\n",
    "pages = int(pages)\n",
    "url = f\"https://www.ptt.cc/bbs/{board}/index.html\"\n",
    "requests.get(url)\n",
    "tl,nl,dl=[],[],[]\n",
    "for page in range(pages):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text,\"html.parser\")\n",
    "    div_t = soup.find_all('div',class_=\"title\")\n",
    "    div_n = soup.find_all('div',class_=\"nrec\")\n",
    "    div_d = soup.find_all('div',class_=\"date\")\n",
    "    for t in div_t:   \n",
    "        tl.append(t.text)\n",
    "    for n in div_n:\n",
    "        nl.append(n.text)\n",
    "    for d in div_d:\n",
    "        dl.append(d.text)\n",
    "    btn = soup.select('div.btn-group > a')\n",
    "    up_page_href = btn[3]['href']\n",
    "    next_page_url = 'https://www.ptt.cc' + up_page_href\n",
    "    url = next_page_url\n",
    "    time.sleep(5) if page == 99 else time.sleep(0)\n",
    "    print(f'搜尋第{page+1}頁',end=' ')\n",
    "dataset = {'標題':tl,'推數':nl,'發文日':dl}\n",
    "dictionary = pd.DataFrame(dataset)\n",
    "dictionary.to_csv(f\"ptt{board}.csv\",encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>關鍵字</h1>\n",
    "輸出至 ptt{board}_{key}.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 複合關鍵字，空格隔開"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "keys = list(input(\"請輸入關鍵字以空白間隔:\").split(\" \"))\n",
    "df = pd.read_csv(f\"ptt{board}.csv\")#,header=None,delimiter='\\t',encoding='utf-8-sig')\n",
    "print(type(df))\n",
    "for key in keys:\n",
    "    mask = df['標題'].str.contains(key)\n",
    "    df[mask].to_csv(f'ptt{board}_{key}.csv',encoding='utf-8-sig',index=False)\n",
    "    with open(f\"ptt{board}_{key}.csv\",'a',encoding='utf-8-sig') as file:\n",
    "        file.write(\"共%d筆\"%len(df[mask]))\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.unicode.ambiguous_as_wide', True)\n",
    "pd.set_option('display.unicode.east_asian_width', True)\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "for key in keys:\n",
    "    data = pd.read_csv(f'ptt{board}_{key}.csv',index_col=None)\n",
    "    dataset = pd.concat([dataset,data])\n",
    "dataset= dataset.drop_duplicates(['標題'])\n",
    "dataset.to_csv(f\"ptt{board}_{keys}.csv\",encoding='utf-8-sig',index=False)\n",
    "with open(f\"ptt{board}_{keys}.csv\",'a',encoding='utf-8-sig') as file:\n",
    "    file.write(\"共%d筆\"%len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多層單關鍵字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def MultiFilter(result):\n",
    "    times = input('輸入還需要篩選幾次 : ')\n",
    "    times = eval(times)\n",
    "    if times > 0 :\n",
    "        for i in range(times):\n",
    "            key = input(f'輸入第{i+1}篩選字 : ')\n",
    "            mask = result[0].str.contains(key)\n",
    "            result = result[mask]\n",
    "            result.to_csv(f\"ptt{board}_{key}.csv\",header=['標題'],encoding='utf-8-sig')\n",
    "            with open(f\"ptt{board}_{key}.csv\",'a',encoding='utf-8-sig') as file:\n",
    "                file.write(\"\\n共%d筆\"%len(result))\n",
    "            print(result)\n",
    "            print(\"共%d筆\"%len(result))\n",
    "        \n",
    "key = input(\"請輸入關鍵字:\")\n",
    "mask = df[0].str.contains(key)\n",
    "result = df[mask]\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.unicode.ambiguous_as_wide', True)\n",
    "pd.set_option('display.unicode.east_asian_width', True)\n",
    "result.to_csv(f\"ptt{board}_{key}.csv\",encoding='utf-8-sig',header=['標題'])\n",
    "with open(f\"ptt{board}_{key}.csv\",'a',encoding='utf-8-sig') as file:\n",
    "    file.write(\"\\n共%d筆\"%len(result))\n",
    "print(result)\n",
    "print(\"共%d筆\"%len(result))\n",
    "MultiFilter(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>看指定文章</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可輸入關鍵字但只顯示同關鍵字最先的一篇\n",
    "\n",
    "輸出至 ptt{board}_{target}.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "target = input(\"請輸入標題:\")\n",
    "url = f\"https://www.ptt.cc/bbs/{board}/index.html\"\n",
    "\n",
    "def GetContent():\n",
    "    ar = soup.select('div.title > a')\n",
    "    ar_href = ar[number]['href']\n",
    "    ar_url = 'https://www.ptt.cc' + ar_href\n",
    "    aurl = requests.get(ar_url)\n",
    "    asoup = BeautifulSoup(aurl.text,\"html.parser\")\n",
    "    CONTENT = asoup.find(id='main-container').text\n",
    "    return file.write(ar_url+CONTENT)\n",
    "    \n",
    "task = 'continue'\n",
    "with open(f\"ptt{board}_{target}.txt\",'w',encoding='utf-8') as file: \n",
    "    for page in range(900,pages):\n",
    "        if task == 'continue':\n",
    "            r = requests.get(url)\n",
    "            soup = BeautifulSoup(r.text,\"html.parser\")\n",
    "            div = soup.find_all('div',class_=\"title\")\n",
    "            number = -1\n",
    "            for t in div:\n",
    "                number += 1\n",
    "                if target in t.text:\n",
    "                    print('Find')\n",
    "                    GetContent()\n",
    "                    task = 'complete'\n",
    "            if task == 'continue':\n",
    "                btn = soup.select('div.btn-group > a')\n",
    "                up_page_href = btn[3]['href']\n",
    "                next_page_url = 'https://www.ptt.cc' + up_page_href\n",
    "                url = next_page_url\n",
    "                print(f'搜尋第{page+1}頁',end='')\n",
    "        else:\n",
    "            print('task ',task)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可輸入關鍵字找全部頁面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "target = input(\"請輸入標題:\")\n",
    "url = f\"https://www.ptt.cc/bbs/{board}/index.html\"\n",
    "\n",
    "def GetContent():\n",
    "    ar = soup.select('div.title > a')\n",
    "    ar_href = ar[number]['href']\n",
    "    ar_url = 'https://www.ptt.cc' + ar_href\n",
    "    aurl = requests.get(ar_url)\n",
    "    asoup = BeautifulSoup(aurl.text,\"html.parser\")\n",
    "    CONTENT = asoup.find(id='main-container').text\n",
    "    file.write(ar_url+CONTENT+\"\\n\")\n",
    "    return ar_url,CONTENT\n",
    "\n",
    "with open(f\"ptt{board}_{target}.txt\",'w',encoding='utf-8') as file: \n",
    "    for page in range(pages):\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text,\"html.parser\")\n",
    "        div = soup.find_all('div',class_=\"title\")\n",
    "        number = -1\n",
    "        for t in div:\n",
    "            number += 1\n",
    "            if target in t.text:\n",
    "                print('Find')\n",
    "                GetContent()\n",
    "        btn = soup.select('div.btn-group > a')\n",
    "        up_page_href = btn[3]['href']\n",
    "        next_page_url = 'https://www.ptt.cc' + up_page_href\n",
    "        url = next_page_url\n",
    "        print(f'搜尋第{page+1}頁',end='')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selenium 運用ptt搜尋功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import bs4\n",
    "import time\n",
    "import pandas as pd\n",
    "\"\"\"\n",
    "施工中\n",
    "\"\"\"\n",
    "target = input(\"輸入文章標題:\")\n",
    "url = f\"https://www.ptt.cc/bbs/{board}/index.html\"\n",
    "browser = webdriver.Chrome(\"chromedriver.exe\")\n",
    "browser.get(url)\n",
    "\n",
    "html = browser.page_source\n",
    "soup = bs4.BeautifulSoup(html,\"html.parser\")\n",
    "div = soup.find_all('div',class_=\"search-bar\")\n",
    "print(div[1])\n",
    "for t in div:\n",
    "    if target in t.text:\n",
    "        browser.find_element_by_link_text(target).click()\n",
    "        print('Find!')\n",
    "        time.sleep(100)\n",
    "    else:\n",
    "        print(\"no\",end = ',')\n",
    "try :\n",
    "    print(i)\n",
    "    buttom = browser.find_element_by_xpath(\"/html/body/div[2]/div[1]/div[1]/div[2]/a[2]\")\n",
    "    buttom.click()\n",
    "except:\n",
    "    break\n",
    "except :\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selenium click in (慢)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import bs4\n",
    "import time\n",
    "import pandas as pd\n",
    "target = input()\n",
    "url = f\"https://www.ptt.cc/bbs/{board}/index.html\"\n",
    "browser = webdriver.Edge(\"C:\\python3.7\\edgedriver_win64\\msedgedriver.exe\")\n",
    "browser.get(url)\n",
    "for i in range(pages): \n",
    "    try:\n",
    "        html = browser.page_source\n",
    "        soup = bs4.BeautifulSoup(html,\"html.parser\")\n",
    "        div = soup.find_all('div',class_=\"title\")\n",
    "        for t in div:\n",
    "            if target in t.text:\n",
    "                browser.find_element_by_link_text(target).click()\n",
    "                print('Find!')\n",
    "                time.sleep(100)\n",
    "            else:\n",
    "                print(\"no\",end = ',')\n",
    "        try :\n",
    "            print(i)\n",
    "            buttom = browser.find_element_by_xpath(\"/html/body/div[2]/div[1]/div[1]/div[2]/a[2]\")\n",
    "            buttom.click()\n",
    "        except:\n",
    "            break\n",
    "    except :\n",
    "        break\n",
    "browser.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
